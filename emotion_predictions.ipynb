{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "import numpy as np\n",
    "import librosa\n",
    "from feature_extraction import extract_features,get_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved functions and models\n",
    "extract_features_loaded = joblib.load('extract_features.joblib')\n",
    "get_features_loaded = joblib.load('get_features.joblib')\n",
    "female_model = load_model('emotion_models/female_model.h5')\n",
    "male_model = load_model('emotion_models/male_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_path, duration=3, offset=0.5, res_type='kaiser_fast'):\n",
    "    \"\"\"Predicts emotion from audio file using female and male models.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        duration (float): Duration of audio to load in seconds.\n",
    "        offset (float): Offset from the beginning of the audio file to load in seconds.\n",
    "        res_type (str): Resampling type.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (predicted_emotion, confidence, speaker)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio and preprocess\n",
    "        audio_data, sr = librosa.load(audio_path, duration=duration, offset=offset, res_type=res_type)\n",
    "        preprocessed_data = extract_features_loaded(audio_data)\n",
    "\n",
    "        # Identify speaker (replace with your speaker identification logic)\n",
    "        # For demo, assuming random prediction for simplicity\n",
    "        speaker = np.random.choice([\"female\", \"male\"])\n",
    "\n",
    "        # Get predictions from female and male models\n",
    "        female_prediction = female_model.predict(np.expand_dims(preprocessed_data, axis=0))[0]\n",
    "        male_prediction = male_model.predict(np.expand_dims(preprocessed_data, axis=0))[0]\n",
    "        \n",
    "        # Calculate confidence\n",
    "        female_confidence = female_prediction.max()\n",
    "        male_confidence = male_prediction.max()\n",
    "        total_confidence = female_confidence + male_confidence\n",
    "        female_weight = female_confidence / total_confidence\n",
    "        male_weight = male_confidence / total_confidence\n",
    "        confidence = female_weight * female_confidence + male_weight * male_confidence\n",
    "\n",
    "        # Choose prediction based on speaker (replace with your logic)\n",
    "        if speaker == \"female\":\n",
    "            predicted_class = np.argmax(female_prediction)\n",
    "        else:\n",
    "            predicted_class = np.argmax(male_prediction)\n",
    "\n",
    "        # Map predicted class to emotion label (replace with your mapping)\n",
    "        emotion_labels = [\"neutral\", \"angry\", \"happy\", \"sad\"]  # Example labels\n",
    "        predicted_emotion = emotion_labels[predicted_class]\n",
    "\n",
    "        return predicted_emotion, confidence, speaker\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Audio file not found at {audio_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    return None, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from tkinter import filedialog\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "\n",
    "def record_audio():\n",
    "    # Function to record audio for 5 seconds\n",
    "    fs = 44100  # Sample rate\n",
    "    seconds = 3  # Duration of recording\n",
    "\n",
    "    print(\"Recording...\")\n",
    "    my_recording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording finished.\")\n",
    "    \n",
    "    # Save the recorded audio as a WAV file\n",
    "    file_path = 'recorded_audio.wav'\n",
    "    sf.write(file_path, my_recording, fs)\n",
    "    print(f\"Audio saved as {file_path}\")\n",
    "    \n",
    "    # Display the waveform\n",
    "    plot_waveform(file_path)\n",
    "\n",
    "def choose_audio_file():\n",
    "    # Function to choose an audio file from the directory\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"WAV files\", \"*.wav\")])\n",
    "    if file_path:\n",
    "        # Display the waveform of the selected audio file\n",
    "        plot_waveform(file_path)\n",
    "\n",
    "def plot_waveform(file_path):\n",
    "    # Function to plot the waveform of the audio file\n",
    "    data, fs = sf.read(file_path)\n",
    "    duration = len(data) / fs\n",
    "    time = [i / fs for i in range(len(data))]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(time, data)\n",
    "    ax.set(xlabel='Time (s)', ylabel='Amplitude', title='Audio Waveform')\n",
    "    ax.grid()\n",
    "\n",
    "    # Embed the plot in the tkinter window\n",
    "    canvas = FigureCanvasTkAgg(fig, master=root)\n",
    "    canvas.draw()\n",
    "    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)\n",
    "\n",
    "    # Update the audio file path\n",
    "    predict_button.audio_path = file_path\n",
    "\n",
    "def predict_and_display():\n",
    "    # Function to predict emotion and display the result\n",
    "    if hasattr(predict_button, 'audio_path'):\n",
    "        audio_path = predict_button.audio_path\n",
    "        predicted_emotion, confidence, speaker = predict_emotion(audio_path)\n",
    "\n",
    "        if predicted_emotion:\n",
    "            result_label.config(text=f\"Predicted emotion: {predicted_emotion} (confidence: {confidence:.2f}, speaker: {speaker})\")\n",
    "        else:\n",
    "            result_label.config(text=\"Error during prediction.\")\n",
    "    else:\n",
    "        result_label.config(text=\"No audio file selected.\")\n",
    "\n",
    "# Create the GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Audio Emotion Predictor\")\n",
    "\n",
    "# Create buttons for recording audio and choosing an audio file\n",
    "record_button = tk.Button(root, text=\"Record Audio\", command=record_audio)\n",
    "record_button.pack(pady=5)\n",
    "\n",
    "choose_button = tk.Button(root, text=\"Choose Audio File\", command=choose_audio_file)\n",
    "choose_button.pack(pady=5)\n",
    "\n",
    "# Create a button to predict emotion from the audio\n",
    "predict_button = tk.Button(root, text=\"Predict Emotion\", command=predict_and_display)\n",
    "predict_button.pack(pady=5)\n",
    "\n",
    "# Create a label to display prediction result\n",
    "result_label = tk.Label(root, text=\"\")\n",
    "result_label.pack(pady=5)\n",
    "\n",
    "# Run the GUI application\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
